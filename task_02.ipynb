{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMnZDy4oT40KnCFG1b33Ic2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize"
      ],
      "metadata": {
        "id": "3lzYptk8YhQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the repo\n",
        "!git clone https://fas38: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@github.com/fas38/nnti-project-25.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6Vz1ixnTucc",
        "outputId": "7e735e66-3b32-4286-bcf4-a9d70e294e8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nnti-project-25'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 38 (delta 13), reused 17 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (38/38), 1.64 MiB | 4.00 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set path\n",
        "import os\n",
        "%cd /content/nnti-project-25/\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "wmaTcnvQEA8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47f1ace9-f4bf-41b9-bd5b-5820a02b0982"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nnti-project-25\n",
            "/content/nnti-project-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZcvcu2FELD0",
        "outputId": "adaf623c-9da0-4f08-beb1-fc2b1171d6bc",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (7.7.1)\n",
            "Collecting jupyter (from -r requirements.txt (line 2))\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.5.1+cu124)\n",
            "Collecting datasets (from -r requirements.txt (line 7))\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.48.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.6.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (0.19.7)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->-r requirements.txt (line 1)) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->-r requirements.txt (line 1)) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->-r requirements.txt (line 1)) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->-r requirements.txt (line 1)) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->-r requirements.txt (line 1)) (3.0.13)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 2)) (6.5.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 2)) (7.16.6)\n",
            "Collecting jupyterlab (from jupyter->-r requirements.txt (line 2))\n",
            "  Downloading jupyterlab-4.3.5-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 4)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 4)) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 7))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (4.67.1)\n",
            "Collecting xxhash (from datasets->-r requirements.txt (line 7))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->-r requirements.txt (line 7))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 8)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 8)) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 8)) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (3.5.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (2.10.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 10)) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 10)) (4.0.12)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (6.4.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 10)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 7)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 7)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 7)) (2025.1.31)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (5.10.4)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 2)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (1.5.1)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 2)) (0.28.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 2)) (0.2.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 2)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 10)) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (0.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (0.8.4)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1))\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter->-r requirements.txt (line 2)) (21.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (0.4)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 2)) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 2)) (4.23.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook->jupyter->-r requirements.txt (line 2)) (2.21.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 2)) (2.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 2)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 2)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 2)) (0.23.1)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading python_json_logger-3.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r requirements.txt (line 2)) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r requirements.txt (line 2)) (2.22)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2))\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.3.5-py3-none-any.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-3.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: xxhash, uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, json5, jedi, fqdn, dill, async-lru, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jupyter-server-terminals, jupyter-client, arrow, nvidia-cusolver-cu12, isoduration, datasets, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.24.0\n",
            "    Uninstalling jupyter-server-1.24.0:\n",
            "      Successfully uninstalled jupyter-server-1.24.0\n",
            "Successfully installed arrow-1.3.0 async-lru-2.0.4 datasets-3.3.2 dill-0.3.8 fqdn-1.5.1 isoduration-20.11.0 jedi-0.19.2 json5-0.10.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.3.5 jupyterlab-server-2.27.3 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 overrides-7.7.0 python-json-logger-3.2.1 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 types-python-dateutil-2.9.0.20241206 uri-template-1.3.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset as HF_Dataset\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "ts0KumnWFv68"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "vIPexXD5A1we"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up Model and Data"
      ],
      "metadata": {
        "id": "-yESRuLjY4z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"  #MoLFormer model"
      ],
      "metadata": {
        "id": "lQo5XRtMFn8S"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pre-trained model from HuggingFace\n",
        "model = AutoModel.from_pretrained(MODEL_NAME, deterministic_eval=True, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "mDAkqkmqFsDW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model with regression head\n",
        "# class MoLFormerWithRegressionHead(nn.Module):\n",
        "#     def __init__(self, base_model):\n",
        "#         super().__init__()\n",
        "#         self.base_model = base_model\n",
        "#         hidden_size = base_model.config.hidden_size\n",
        "#         self.regression_head = nn.Linear(hidden_size, 1)\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask=None):\n",
        "#         outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "#         return self.regression_head(pooled_output)\n",
        "\n",
        "class MoLFormerWithRegressionHead(nn.Module):\n",
        "    # TODO: your code goes here\n",
        "  def __init__(self,model):\n",
        "    super(MoLFormerWithRegressionHead, self).__init__()\n",
        "    self.pretrained = model\n",
        "    hidden_size = self.pretrained.config.hidden_size\n",
        "\n",
        "    #dropout?\n",
        "\n",
        "    self.regression = nn.Linear(hidden_size, 1)\n",
        "\n",
        "\n",
        "  def forward(self, ids, mask):\n",
        "    # pass input to the pre-trained model\n",
        "    output = self.pretrained(ids, attention_mask=mask)\n",
        "    # extracts the last hidden state\n",
        "    hidden_states = output.last_hidden_state\n",
        "    # selects the cls token, represents the summary of the entire sequence\n",
        "    cls_representation = hidden_states[:, 0, :]\n",
        "\n",
        "    output = self.regression(cls_representation)\n",
        "    return output.squeeze(-1) # to remove the last dimension\n",
        "\n"
      ],
      "metadata": {
        "id": "Jk8ob2WrHTqL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model\n",
        "regression_model = MoLFormerWithRegressionHead(model).to(device)"
      ],
      "metadata": {
        "id": "OHi3EuvoH0kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset"
      ],
      "metadata": {
        "id": "3moGw3ldZM38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class SMILESDataset(Dataset):\n",
        "#     def __init__(self, smiles_list, targets, tokenizer, max_length=128):\n",
        "#         self.smiles_list = smiles_list\n",
        "#         self.targets = targets\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_length = max_length\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.smiles_list)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         smiles = self.smiles_list[idx]\n",
        "#         target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
        "\n",
        "#         # Tokenize the SMILES string\n",
        "#         encoding = self.tokenizer(\n",
        "#             smiles,\n",
        "#             max_length=self.max_length,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             return_tensors=\"pt\"\n",
        "#         )\n",
        "\n",
        "#         return {\n",
        "#             \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "#             \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "#             \"target\": target\n",
        "#         }\n",
        "\n",
        "class SMILESDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data, tokenizer, max_length):\n",
        "      self.data = data\n",
        "      self.tokenizer = tokenizer\n",
        "      self.max_len = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      row = self.data[idx]\n",
        "      # row = self.data.iloc[idx]\n",
        "      # row = self.data if isinstance(self.data, dict) else self.data[idx]\n",
        "      SMILES = row['SMILES']\n",
        "      label = row['label']\n",
        "\n",
        "      inputs = self.tokenizer.encode_plus(\n",
        "      SMILES,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      padding='max_length',\n",
        "      return_token_type_ids=False,\n",
        "      truncation=True\n",
        "  )\n",
        "\n",
        "      return {\n",
        "    'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
        "    'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
        "    'target': torch.tensor(label, dtype=torch.float)  # Directly convert the target to float\n",
        "}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "DATASET_PATH = \"scikit-fingerprints/MoleculeNet_Lipophilicity\"\n",
        "dataset = load_dataset(DATASET_PATH)\n",
        "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "train_valid_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n",
        "# Then, split train_valid_dataset into train and validation (e.g., 90/10 of train_valid_dataset)\n",
        "split_train_valid = train_valid_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split_train_valid[\"train\"]\n",
        "valid_dataset = split_train_valid[\"test\"]\n",
        "train_dataset = SMILESDataset(train_dataset, tokenizer, max_length=128)\n",
        "valid_dataset = SMILESDataset(valid_dataset, tokenizer, max_length=128)\n",
        "test_dataset  = SMILESDataset(test_dataset, tokenizer, max_length=128)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "test_dataloader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# # loading dataset\n",
        "# DATASET_PATH = \"scikit-fingerprints/MoleculeNet_Lipophilicity\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "# dataset = load_dataset(DATASET_PATH)\n",
        "# df = pd.DataFrame(dataset[\"train\"])\n",
        "# smiles_list = df[\"SMILES\"].tolist()\n",
        "# targets = df[\"label\"].tolist()\n",
        "# smiles_dataset = SMILESDataset(smiles_list, targets, tokenizer)\n",
        "\n",
        "# # creating train-test split\n",
        "# train_size = int(0.7 * len(smiles_dataset))\n",
        "# test_size = len(smiles_dataset) - train_size\n",
        "# train_dataset, test_dataset = torch.utils.data.random_split(smiles_dataset, [train_size, test_size])\n",
        "\n",
        "# # creating dataloaders\n",
        "# batch_size = 16\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "C5Y_Zj9wJ6sx"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Regression Model"
      ],
      "metadata": {
        "id": "aAREquunaFmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "NMtsyx9maLZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(regression_model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 5\n",
        "regression_model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        targets = batch[\"target\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = regression_model(input_ids, attention_mask).squeeze()\n",
        "        loss = criterion(predictions, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "J-DS4gjMKcLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating"
      ],
      "metadata": {
        "id": "8xp7C0JqaNJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regression_model.eval()\n",
        "total_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        targets = batch[\"target\"].to(device)\n",
        "\n",
        "        predictions = regression_model(input_ids, attention_mask).squeeze()\n",
        "        loss = criterion(predictions, targets)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_dataloader)\n",
        "    print(f\"Test Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aSnHyoeab_O",
        "outputId": "92430b00-94e7-43b9-986b-48739a305acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.4409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and Load"
      ],
      "metadata": {
        "id": "W2UF7Yfc9-Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save model to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/Colab Notebooks/nnti/'\n",
        "os.chdir(path)\n",
        "torch.save(regression_model.state_dict(), \"regression_model_dummy.pth\")\n",
        "\n",
        "# reset the path to git repo\n",
        "os.chdir(\"/content/nnti-project-25/\")\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "qpSdfpSF-B75",
        "outputId": "2267d5c5-9fdf-4412-b07f-e641b8269771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading saved model\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# path = '/content/drive/My Drive/Colab Notebooks/nnti/'\n",
        "# os.chdir(path)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# regression_model = MoLFormerWithRegressionHead(model).to(device)\n",
        "# regression_model.load_state_dict(torch.load(\"regression_model_dummy.pth\"))\n",
        "# regression_model.eval()\n",
        "\n",
        "# loading pretrained masked model\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/Colab Notebooks/nnti/'\n",
        "os.chdir(path)\n",
        "mlm_finetuned_model = AutoModel.from_pretrained(\"./mlm_finetuned_model\", local_files_only=True, trust_remote_code=True).to(device)\n",
        "mlm_regression_model = MoLFormerWithRegressionHead(mlm_finetuned_model).to(device)\n",
        "# mlm_regression_model.eval()\n",
        "\n",
        "# reset the path to git repo\n",
        "os.chdir(\"/content/nnti-project-25/\")\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxD1p1uY-Evu",
        "outputId": "c28bc592-088b-4533-f9fa-7ea1dba3340c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nnti-project-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Influence Function"
      ],
      "metadata": {
        "id": "4cERJEhdbfRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resource"
      ],
      "metadata": {
        "id": "1TjWMcrKBVAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# place to start - https://github.com/nimarb/pytorch_influence_functions\n",
        "# https://github.com/xbeat/Machine-Learning/blob/main/Second-Order%20Equations%20in%20Machine%20Learning%20Algorithms%20Using%20Python.md"
      ],
      "metadata": {
        "id": "eQGK2B_PcQ-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading external dataset"
      ],
      "metadata": {
        "id": "L8Y-pvXuBYYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading external dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "DATASET_PATH = \"scikit-fingerprints/MoleculeNet_Lipophilicity\"\n",
        "dataset = load_dataset(DATASET_PATH)\n",
        "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "train_valid_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n",
        "# Then, split train_valid_dataset into train and validation (e.g., 90/10 of train_valid_dataset)\n",
        "split_train_valid = train_valid_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split_train_valid[\"train\"]\n",
        "valid_dataset = split_train_valid[\"test\"]\n",
        "train_dataset = SMILESDataset(train_dataset, tokenizer, max_length=128)\n",
        "valid_dataset = SMILESDataset(valid_dataset, tokenizer, max_length=128)\n",
        "test_dataset  = SMILESDataset(test_dataset, tokenizer, max_length=128)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "test_dataloader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "ext_data = pd.read_csv(\"./tasks/External-Dataset_for_Task2.csv\")\n",
        "ext_data = ext_data.iloc[0:5]\n",
        "ext_data = ext_data.rename(columns={\"Label\": \"label\"})\n",
        "ext_dataset = HF_Dataset.from_pandas(ext_data)\n",
        "ext_dataset = ext_dataset.remove_columns([\"__index__\"]) if \"__index__\" in ext_dataset.column_names else ext_dataset\n",
        "# ext_smiles_list = ext_data[\"SMILES\"].tolist()\n",
        "# ext_targets = ext_data[\"Label\"].tolist()\n",
        "# ext_dataset = SMILESDataset(ext_smiles_list, ext_targets, tokenizer)\n",
        "ext_dataset = SMILESDataset(ext_dataset, tokenizer, max_length=128)\n",
        "\n",
        "# creating dataloaders\n",
        "batch_size = 1\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "ext_dataloader = DataLoader(ext_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "mej6mVcDgUp7"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## computation functions"
      ],
      "metadata": {
        "id": "XWSOJdz6B2QX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### for single test sample"
      ],
      "metadata": {
        "id": "HGOs3bj7Y5kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hvp(model, loss, v, max_norm=1.0):\n",
        "    \"\"\"\n",
        "    Computes the Hessian-vector product (HVP)\n",
        "\n",
        "    Parameters:\n",
        "    - model: Pre-trained model\n",
        "    - loss: MSE Output\n",
        "    - v: Gradient vector\n",
        "    - max_norm: Maximum allowed norm for HVP.\n",
        "\n",
        "    Returns:\n",
        "    - The Hessian-vector product (HVP)\n",
        "    \"\"\"\n",
        "    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True, retain_graph=True)\n",
        "    flat_grads = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "    hvp = torch.autograd.grad(flat_grads @ v, model.parameters(), retain_graph=True)\n",
        "    hvp_flat = torch.cat([h.view(-1) for h in hvp])\n",
        "\n",
        "    # clipping\n",
        "    hvp_norm = torch.norm(hvp_flat, p=2) # L2 norm\n",
        "    if hvp_norm > max_norm:\n",
        "        hvp_flat = hvp_flat * (max_norm / hvp_norm)\n",
        "\n",
        "    # Debugging: check for explosion\n",
        "    if torch.norm(hvp_flat) > 1e6:\n",
        "        print(f\"\\nExploding values detected in HVP after clipping! Norm: {torch.norm(hvp_flat)}\")\n",
        "\n",
        "    return hvp_flat\n",
        "\n",
        "\n",
        "def lissa_approximation(model, train_dataloader, v, damping=0.1, num_samples=5, num_iter=100, num_repeats=5, criterion=None):\n",
        "    \"\"\"\n",
        "    Approximates Hessian-inverse-vector product (iHVP) using the stochastic estimation method\n",
        "    explained in https://arxiv.org/pdf/1703.04730 and https://arxiv.org/pdf/1602.03943\n",
        "\n",
        "    Parameters:\n",
        "    - model: Pre-trained model\n",
        "    - train_dataloader: Dataloader for training data\n",
        "    - v: Gradient vector\n",
        "    - damping: Damping factor for stabilization\n",
        "    - num_samples: Number of training points (t) to sample per iteration\n",
        "    - num_iter: Number of Taylor approximation iterations\n",
        "    - num_repeats: Number of times to repeat estimation to reduce variance (r)\n",
        "\n",
        "    Returns:\n",
        "    - Approximate inverse Hessian-vector product (iHVP)\n",
        "    \"\"\"\n",
        "    ihvp_estimates = []\n",
        "\n",
        "    for i in range(num_repeats):\n",
        "        # H^{-1}_0 v = v\n",
        "        print(f\"Repeat {i+1}/{num_repeats}\")\n",
        "        z = v.clone()\n",
        "\n",
        "        # sampling training points\n",
        "        indices = torch.randint(len(train_dataloader.dataset), (num_samples,))\n",
        "        sampled_train_data = [train_dataloader.dataset[i] for i in indices]\n",
        "\n",
        "        # taylor approximation\n",
        "        for j in range(num_iter):\n",
        "            print(f\"Iteration {j+1}/{num_iter}\")\n",
        "            train_batch = sampled_train_data[j % num_samples] # Filtering the sampled train instances\n",
        "            train_input_ids = torch.stack([item['input_ids'] for item in sampled_train_data]).to(device)\n",
        "            train_attention_mask = torch.stack([item['attention_mask'] for item in sampled_train_data]).to(device)\n",
        "            train_label = torch.stack([item['target'] for item in sampled_train_data]).to(device)\n",
        "\n",
        "            # Compute Hessian-gradient product using the sampled loss\n",
        "            train_loss = criterion(\n",
        "                model(train_input_ids, train_attention_mask).view(-1), train_label\n",
        "            )\n",
        "            hvp = compute_hvp(model, train_loss, z)\n",
        "\n",
        "            # update: H_j^{-1} v = v + (I - H) H_{j-1}^{-1} v\n",
        "            z = v + (z - hvp)\n",
        "\n",
        "        ihvp_estimates.append(z)\n",
        "\n",
        "    return torch.stack(ihvp_estimates).mean(dim=0)\n",
        "\n",
        "def influence_function(train_point, train_label, test_point, test_label, model, criterion, train_dataloader, num_samples=5, num_iter=5, num_repeats=5):\n",
        "    \"\"\"\n",
        "    Computes the influence of a training point on a single test point using the stochastic method\n",
        "\n",
        "    Parameters:\n",
        "    - train_point: Dictionary containing {'input_ids': tensor, 'attention_mask': tensor}\n",
        "    - train_label: Target label for the training point\n",
        "    - test_point: Dictionary containing {'input_ids': tensor, 'attention_mask': tensor}\n",
        "    - test_label: Target label for the test point\n",
        "    - model: Pre-trained model\n",
        "    - criterion: Loss function (MSE)\n",
        "    - train_dataloader: Dataloader for training data\n",
        "    - num_samples: Number of training points (t) to sample per iteration\n",
        "    - num_iter: Number of Taylor approximation iterations\n",
        "    - num_repeats: Number of times to repeat estimation (r)\n",
        "\n",
        "    Returns:\n",
        "    - Influence of the training point on the test point\n",
        "    \"\"\"\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Compute gradient of test loss w.r.t. model parameters\n",
        "    test_loss = criterion(model(test_point['input_ids'], test_point['attention_mask']).view(-1), test_label)\n",
        "    grad_test = torch.autograd.grad(test_loss, model.parameters(), retain_graph=True)\n",
        "    grad_test_vector = torch.cat([g.view(-1) for g in grad_test])\n",
        "\n",
        "    # Compute gradient of training loss w.r.t. model parameters\n",
        "    train_loss = criterion(model(train_point['input_ids'], train_point['attention_mask']).view(-1), train_label)\n",
        "    grad_train = torch.autograd.grad(train_loss, model.parameters(), retain_graph=True)\n",
        "    grad_train_vector = torch.cat([g.view(-1) for g in grad_train])\n",
        "\n",
        "    # Compute Hessian-inverse-vector product using LiSSA\n",
        "    print(\"computing ihvp\")\n",
        "    ihvp = lissa_approximation(model, train_dataloader, grad_train_vector, num_samples=num_samples, num_iter=num_iter, num_repeats=num_repeats, criterion=criterion)\n",
        "\n",
        "    # Compute influence\n",
        "    influence = torch.dot(grad_test_vector, ihvp)\n",
        "\n",
        "    return -influence\n",
        "\n"
      ],
      "metadata": {
        "id": "hiZ5MXqlT_o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### driver code"
      ],
      "metadata": {
        "id": "KX4TlnspuHXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "influences = []\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "test_batch = next(iter(test_dataloader))\n",
        "test_point = {\n",
        "    'input_ids': test_batch['input_ids'].to(device),\n",
        "    'attention_mask': test_batch['attention_mask'].to(device)\n",
        "}\n",
        "test_label = test_batch['target'].to(device)\n",
        "\n",
        "# Compute influence for each training point\n",
        "for train_batch in ext_dataloader:\n",
        "    print(f\"External data sample {len(influences) + 1}/{len(ext_dataloader)}\")\n",
        "    train_input_ids = train_batch['input_ids'].to(device)\n",
        "    train_attention_mask = train_batch['attention_mask'].to(device)\n",
        "    train_label = train_batch['target'].to(device)\n",
        "\n",
        "    train_point = {\n",
        "        'input_ids': train_input_ids,\n",
        "        'attention_mask': train_attention_mask\n",
        "    }\n",
        "\n",
        "    # Compute influence\n",
        "    influence = influence_function(\n",
        "        train_point, train_label, test_point, test_label,\n",
        "        regression_model, criterion, train_dataloader\n",
        "    )\n",
        "\n",
        "    influences.append(influence.item())\n",
        "    print(f\"Influence for current training batch: {influence.item()}\")\n",
        "\n",
        "# Rank training points by influence\n",
        "# ranked_indices = sorted(range(len(influences)), key=lambda i: influences[i], reverse=True)\n",
        "ranked_indices = sorted(enumerate(influences), key=lambda x: x[1], reverse=True)\n",
        "print(\"Most influential training points:\", ranked_indices)\n"
      ],
      "metadata": {
        "id": "2JAfznPpuOUA",
        "outputId": "4dc1a330-8485-4ff1-bf2b-5d44a651a127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "External data sample 1/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: -239.3598175048828\n",
            "External data sample 2/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: -199.82586669921875\n",
            "External data sample 3/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: -865.6749267578125\n",
            "External data sample 4/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: 580.6749267578125\n",
            "External data sample 5/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: 207.1547393798828\n",
            "Most influential training points: [(3, 580.6749267578125), (4, 207.1547393798828), (1, -199.82586669921875), (0, -239.3598175048828), (2, -865.6749267578125)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### for full test set"
      ],
      "metadata": {
        "id": "4exrB1F6uQ4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hvp(model, loss, v, max_norm=1.0):\n",
        "    \"\"\"\n",
        "    Computes the Hessian-vector product (HVP)\n",
        "\n",
        "    Parameters:\n",
        "    - model: Pre-trained model\n",
        "    - loss: MSE Output\n",
        "    - v: Gradient vector\n",
        "    - max_norm: Maximum allowed norm for HVP.\n",
        "\n",
        "    Returns:\n",
        "    - The Hessian-vector product (HVP)\n",
        "    \"\"\"\n",
        "    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True, retain_graph=True)\n",
        "    flat_grads = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "    hvp = torch.autograd.grad(flat_grads @ v, model.parameters(), retain_graph=True)\n",
        "    hvp_flat = torch.cat([h.view(-1) for h in hvp])\n",
        "\n",
        "    # clipping\n",
        "    hvp_norm = torch.norm(hvp_flat, p=2) # L2 norm\n",
        "    if hvp_norm > max_norm:\n",
        "        hvp_flat = hvp_flat * (max_norm / hvp_norm)\n",
        "\n",
        "    # Debugging: check for explosion\n",
        "    if torch.norm(hvp_flat) > 1e6:\n",
        "        print(f\"\\nExploding values detected in HVP after clipping! Norm: {torch.norm(hvp_flat)}\")\n",
        "\n",
        "    return hvp_flat\n",
        "\n",
        "\n",
        "def lissa_approximation(model, train_dataloader, v, damping=0.1, num_samples=5, num_iter=100, num_repeats=5, criterion=None):\n",
        "    \"\"\"\n",
        "    Approximates Hessian-inverse-vector product (iHVP) using the stochastic estimation method\n",
        "    explained in https://arxiv.org/pdf/1703.04730 and https://arxiv.org/pdf/1602.03943\n",
        "\n",
        "    Parameters:\n",
        "    - model: Pre-trained model\n",
        "    - train_dataloader: Dataloader for training data\n",
        "    - v: Gradient vector\n",
        "    - damping: Damping factor for stabilization\n",
        "    - num_samples: Number of training points (t) to sample per iteration\n",
        "    - num_iter: Number of Taylor approximation iterations\n",
        "    - num_repeats: Number of times to repeat estimation to reduce variance (r)\n",
        "\n",
        "    Returns:\n",
        "    - Approximate inverse Hessian-vector product (iHVP)\n",
        "    \"\"\"\n",
        "    ihvp_estimates = []\n",
        "\n",
        "    for i in range(num_repeats):\n",
        "        # H^{-1}_0 v = v\n",
        "        print(f\"Repeat {i+1}/{num_repeats}\")\n",
        "        z = v.clone()\n",
        "\n",
        "        # sampling training points\n",
        "        print(f\"Dataset length: {len(train_dataloader.dataset)}\")\n",
        "        print(f\"Dataset type: {type(train_dataloader.dataset)}\")\n",
        "        indices = torch.randint(len(train_dataloader.dataset), (num_samples,)).tolist()\n",
        "        sampled_train_data = [train_dataloader.dataset[i] for i in indices]\n",
        "\n",
        "        # taylor approximation\n",
        "        for j in range(num_iter):\n",
        "            print(f\"Iteration {j+1}/{num_iter}\")\n",
        "            train_batch = sampled_train_data[j % num_samples] # Filtering the sampled train instances\n",
        "            train_input_ids = torch.stack([item['input_ids'] for item in sampled_train_data]).to(device)\n",
        "            train_attention_mask = torch.stack([item['attention_mask'] for item in sampled_train_data]).to(device)\n",
        "            train_label = torch.stack([item['target'] for item in sampled_train_data]).to(device)\n",
        "\n",
        "            # Compute Hessian-gradient product using the sampled loss\n",
        "            train_loss = criterion(\n",
        "                model(train_input_ids, train_attention_mask).view(-1), train_label\n",
        "            )\n",
        "            hvp = compute_hvp(model, train_loss, z)\n",
        "\n",
        "            # update: H_j^{-1} v = v + (I - H) H_{j-1}^{-1} v\n",
        "            z = v + (z - hvp)\n",
        "\n",
        "        ihvp_estimates.append(z)\n",
        "\n",
        "    return torch.stack(ihvp_estimates).mean(dim=0)\n",
        "\n",
        "def influence_function(train_point, train_label, grad_test_vector, model, criterion, train_dataloader, num_samples=5, num_iter=5, num_repeats=5):\n",
        "    \"\"\"\n",
        "    Computes the influence of a training point over full test set using stochastic method\n",
        "\n",
        "    Parameters:\n",
        "    - train_point: Dictionary containing {'input_ids': tensor, 'attention_mask': tensor}\n",
        "    - train_label: Target label for the training point\n",
        "    - grad_test_vector: Precomputed gradient of test loss w.r.t. model parameters\n",
        "    - model: Pre-trained model\n",
        "    - criterion: Loss function (MSELoss)\n",
        "    - train_dataloader: Dataloader for training data\n",
        "    - num_samples: Number of training points (t) to sample per iteration\n",
        "    - num_iter: Number of Taylor approximation iterations\n",
        "    - num_repeats: Number of times to repeat estimation (r)\n",
        "\n",
        "    Returns:\n",
        "    - Influence of the training point on the test set\n",
        "    \"\"\"\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Compute gradient of training loss w.r.t. model parameters\n",
        "    train_loss = criterion(model(train_point['input_ids'], train_point['attention_mask']).view(-1), train_label)\n",
        "    grad_train = torch.autograd.grad(train_loss, model.parameters(), retain_graph=True)\n",
        "    grad_train_vector = torch.cat([g.view(-1) for g in grad_train])\n",
        "\n",
        "    # Compute Hessian-inverse-vector product using LiSSA\n",
        "    print(\"computing ihvp\")\n",
        "    ihvp = lissa_approximation(model, train_dataloader, grad_train_vector, num_samples=num_samples, num_iter=num_iter, num_repeats=num_repeats, criterion=criterion)\n",
        "\n",
        "    # Compute influence using dot product\n",
        "    influence = torch.dot(grad_test_vector, ihvp)\n",
        "\n",
        "    return -influence\n"
      ],
      "metadata": {
        "id": "gvcEf65UpThr"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### driver code"
      ],
      "metadata": {
        "id": "aMtuGOrBCDCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS_reg = 20\n",
        "LEARNING_RATE_reg = 1e-7\n",
        "optimizer_reg = torch.optim.Adam(mlm_regression_model.parameters(), lr=LEARNING_RATE_reg)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for epoch in range(EPOCHS_reg):\n",
        "    mlm_regression_model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        mask = batch['attention_mask'].to(device)\n",
        "        targets = batch['target'].to(device)\n",
        "\n",
        "        optimizer_reg.zero_grad()\n",
        "        outputs = mlm_regression_model(input_ids, mask)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer_reg.step()\n",
        "\n",
        "        total_train_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataset)\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation phase inside the epoch loop\n",
        "    mlm_regression_model.eval()\n",
        "    total_valid_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            mask = batch['attention_mask'].to(device)\n",
        "            targets = batch['target'].to(device)\n",
        "            outputs = mlm_regression_model(input_ids, mask)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            total_valid_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_valid_loss = total_valid_loss / len(valid_dataset)\n",
        "    print(f\"Epoch {epoch+1} - Validation Loss: {avg_valid_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "GAEkXZOEmXPw",
        "outputId": "862e6f8d-43b7-498d-84bb-58c25ec4da77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Train Loss: 3.6670\n",
            "Epoch 1 - Validation Loss: 3.3647\n",
            "Epoch 2 - Train Loss: 2.9914\n",
            "Epoch 2 - Validation Loss: 2.7446\n",
            "Epoch 3 - Train Loss: 2.4648\n",
            "Epoch 3 - Validation Loss: 2.3095\n",
            "Epoch 4 - Train Loss: 2.1177\n",
            "Epoch 4 - Validation Loss: 1.9514\n",
            "Epoch 5 - Train Loss: 1.8691\n",
            "Epoch 5 - Validation Loss: 1.7135\n",
            "Epoch 6 - Train Loss: 1.6798\n",
            "Epoch 6 - Validation Loss: 1.5737\n",
            "Epoch 7 - Train Loss: 1.6096\n",
            "Epoch 7 - Validation Loss: 1.5059\n",
            "Epoch 8 - Train Loss: 1.5517\n",
            "Epoch 8 - Validation Loss: 1.4709\n",
            "Epoch 9 - Train Loss: 1.5148\n",
            "Epoch 9 - Validation Loss: 1.4393\n",
            "Epoch 10 - Train Loss: 1.4638\n",
            "Epoch 10 - Validation Loss: 1.4005\n",
            "Epoch 11 - Train Loss: 1.4705\n",
            "Epoch 11 - Validation Loss: 1.4019\n",
            "Epoch 12 - Train Loss: 1.4464\n",
            "Epoch 12 - Validation Loss: 1.3952\n",
            "Epoch 13 - Train Loss: 1.4265\n",
            "Epoch 13 - Validation Loss: 1.3577\n",
            "Epoch 14 - Train Loss: 1.4028\n",
            "Epoch 14 - Validation Loss: 1.3463\n",
            "Epoch 15 - Train Loss: 1.3917\n",
            "Epoch 15 - Validation Loss: 1.2787\n",
            "Epoch 16 - Train Loss: 1.3671\n",
            "Epoch 16 - Validation Loss: 1.2906\n",
            "Epoch 17 - Train Loss: 1.3824\n",
            "Epoch 17 - Validation Loss: 1.3188\n",
            "Epoch 18 - Train Loss: 1.3557\n",
            "Epoch 18 - Validation Loss: 1.2911\n",
            "Epoch 19 - Train Loss: 1.3460\n",
            "Epoch 19 - Validation Loss: 1.2654\n",
            "Epoch 20 - Train Loss: 1.3269\n",
            "Epoch 20 - Validation Loss: 1.2603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "influences = []\n",
        "criterion = nn.MSELoss()\n",
        "regression_model = mlm_regression_model\n",
        "\n",
        "# compute overall test loss\n",
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "test_labels = []\n",
        "print(\"Computing test loss gradient...\")\n",
        "grad_test_accum = None\n",
        "num_test_samples = 0\n",
        "for test_batch in test_dataloader:\n",
        "    batch_input_ids = test_batch['input_ids'].to(device)\n",
        "    batch_attention_mask = test_batch['attention_mask'].to(device)\n",
        "    batch_labels = test_batch['target'].to(device)\n",
        "\n",
        "    test_loss = criterion(regression_model(batch_input_ids, batch_attention_mask).squeeze(), batch_labels)\n",
        "    grad_test = torch.autograd.grad(test_loss, regression_model.parameters(), retain_graph=True)\n",
        "    grad_test_vector = torch.cat([g.view(-1) for g in grad_test])\n",
        "\n",
        "    if grad_test_accum is None:\n",
        "        grad_test_accum = grad_test_vector.clone()\n",
        "    else:\n",
        "        grad_test_accum += grad_test_vector\n",
        "\n",
        "    num_test_samples += batch_labels.shape[0]\n",
        "grad_test_vector = grad_test_accum / num_test_samples\n",
        "\n",
        "# Compute influence for each training point\n",
        "for train_batch in ext_dataloader:\n",
        "    print(f\"External data sample {len(influences) + 1}/{len(ext_dataloader)}\")\n",
        "    train_input_ids = train_batch['input_ids'].to(device)\n",
        "    train_attention_mask = train_batch['attention_mask'].to(device)\n",
        "    train_label = train_batch['target'].to(device)\n",
        "\n",
        "    train_point = {\n",
        "        'input_ids': train_input_ids,\n",
        "        'attention_mask': train_attention_mask\n",
        "    }\n",
        "\n",
        "    # Compute influence\n",
        "    influence = influence_function(\n",
        "        train_point, train_label, grad_test_vector,\n",
        "        regression_model, criterion, train_dataloader\n",
        "    )\n",
        "\n",
        "    influences.append(influence.item())\n",
        "    print(f\"Influence for current training batch: {influence.item()}\")\n",
        "\n",
        "# Rank training points by influence\n",
        "# ranked_indices = sorted(range(len(influences)), key=lambda i: influences[i], reverse=True)\n",
        "ranked_indices = sorted(enumerate(influences), key=lambda x: x[1], reverse=True)\n",
        "print(\"Most influential training points:\", ranked_indices)\n"
      ],
      "metadata": {
        "id": "NdAVwCqYwJ5n",
        "outputId": "523f06b1-5fec-4f7b-afdf-7024058ef99e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing test loss gradient...\n",
            "External data sample 1/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: -20.663576126098633\n",
            "External data sample 2/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: 43.412052154541016\n",
            "External data sample 3/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: -116.74801635742188\n",
            "External data sample 4/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: 59.061702728271484\n",
            "External data sample 5/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: -60.81550216674805\n",
            "Most influential training points: [(3, 59.061702728271484), (1, 43.412052154541016), (0, -20.663576126098633), (4, -60.81550216674805), (2, -116.74801635742188)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Garbage Cleaning"
      ],
      "metadata": {
        "id": "KwlEt-ylDOuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del regression_model\n",
        "del train_dataset\n",
        "del test_dataset\n",
        "del train_dataloader\n",
        "del test_dataloader\n",
        "del ext_dataset\n",
        "del ext_dataloader\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "aqszc-zLCuSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}