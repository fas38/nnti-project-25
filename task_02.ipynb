{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNXph1Ktw1arOQUfNVR85EV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize"
      ],
      "metadata": {
        "id": "3lzYptk8YhQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the repo\n",
        "!git clone https://fas38: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@github.com/fas38/nnti-project-25.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6Vz1ixnTucc",
        "outputId": "bc8dc08b-7929-4a47-aa00-1e8b216c9de5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nnti-project-25'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 41 (delta 15), reused 17 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (41/41), 1.64 MiB | 28.55 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set path\n",
        "import os\n",
        "%cd /content/nnti-project-25/\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "wmaTcnvQEA8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c302feb9-9ead-4f38-82b0-5e91a8878c5f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nnti-project-25\n",
            "/content/nnti-project-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZcvcu2FELD0",
        "outputId": "cdd0ee31-e8cb-4259-9d3d-318ae91db007",
        "collapsed": true
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (7.7.1)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.5.1+cu124)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (3.3.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.48.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.6.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (0.19.7)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->-r requirements.txt (line 1)) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->-r requirements.txt (line 1)) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->-r requirements.txt (line 1)) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->-r requirements.txt (line 1)) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->-r requirements.txt (line 1)) (3.0.13)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 2)) (6.5.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 2)) (7.16.6)\n",
            "Requirement already satisfied: jupyterlab in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 2)) (4.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 4)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 4)) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 7)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 8)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 8)) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 8)) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (3.5.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (2.10.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 10)) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 10)) (4.0.12)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (6.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 10)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 7)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 7)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 7)) (2025.1.31)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (5.10.4)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 2)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 2)) (1.5.1)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 2)) (2.0.4)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 2)) (0.28.1)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 2)) (2.2.5)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 2)) (2.15.0)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 2)) (2.27.3)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 2)) (0.2.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 2)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 10)) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (0.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (0.8.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 1)) (0.4)\n",
            "Requirement already satisfied: jupyter-events>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter->-r requirements.txt (line 2)) (21.2.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 2)) (2.17.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 2)) (4.23.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook->jupyter->-r requirements.txt (line 2)) (2.21.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 1)) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 2)) (2.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 2)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 2)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 2)) (0.23.1)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (0.1.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r requirements.txt (line 2)) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r requirements.txt (line 2)) (2.22)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (24.11.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 2)) (2.9.0.20241206)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from datasets import Dataset as HF_Dataset\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "ts0KumnWFv68"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "vIPexXD5A1we"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class and Methods"
      ],
      "metadata": {
        "id": "5kzNKy2482eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model class with regression head\n",
        "class MoLFormerWithRegressionHead(nn.Module):\n",
        "    # TODO: your code goes here\n",
        "  def __init__(self,model):\n",
        "    super(MoLFormerWithRegressionHead, self).__init__()\n",
        "    self.pretrained = model\n",
        "    hidden_size = self.pretrained.config.hidden_size\n",
        "\n",
        "    #dropout?\n",
        "\n",
        "    self.regression = nn.Linear(hidden_size, 1)\n",
        "\n",
        "\n",
        "  def forward(self, ids, mask):\n",
        "    # pass input to the pre-trained model\n",
        "    output = self.pretrained(ids, attention_mask=mask)\n",
        "    # extracts the last hidden state\n",
        "    hidden_states = output.last_hidden_state\n",
        "    # selects the cls token, represents the summary of the entire sequence\n",
        "    cls_representation = hidden_states[:, 0, :]\n",
        "\n",
        "    output = self.regression(cls_representation)\n",
        "    return output.squeeze(-1) # to remove the last dimension\n",
        "\n",
        "# dataset class\n",
        "class SMILESDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data, tokenizer, max_length):\n",
        "      self.data = data\n",
        "      self.tokenizer = tokenizer\n",
        "      self.max_len = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      row = self.data[idx]\n",
        "      # row = self.data.iloc[idx]\n",
        "      # row = self.data if isinstance(self.data, dict) else self.data[idx]\n",
        "      SMILES = row['SMILES']\n",
        "      label = row['label']\n",
        "\n",
        "      inputs = self.tokenizer.encode_plus(\n",
        "      SMILES,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      padding='max_length',\n",
        "      return_token_type_ids=False,\n",
        "      truncation=True\n",
        "  )\n",
        "\n",
        "      return {\n",
        "    'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
        "    'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
        "    'target': torch.tensor(label, dtype=torch.float)  # Directly convert the target to float\n",
        "}"
      ],
      "metadata": {
        "id": "zi9ZNUTB9A43"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods for Computing Influence"
      ],
      "metadata": {
        "id": "QGBZz_rA9rjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hvp(model, loss, v, max_norm=1.0):\n",
        "    \"\"\"\n",
        "    Computes the Hessian-vector product (HVP)\n",
        "\n",
        "    Parameters:\n",
        "    - model: Pre-trained model\n",
        "    - loss: MSE Output\n",
        "    - v: Gradient vector\n",
        "    - max_norm: Maximum allowed norm for HVP.\n",
        "\n",
        "    Returns:\n",
        "    - The Hessian-vector product (HVP)\n",
        "    \"\"\"\n",
        "    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True, retain_graph=True)\n",
        "    flat_grads = torch.cat([g.view(-1) for g in grads])\n",
        "\n",
        "    hvp = torch.autograd.grad(flat_grads @ v, model.parameters(), retain_graph=True)\n",
        "    hvp_flat = torch.cat([h.view(-1) for h in hvp])\n",
        "\n",
        "    # clipping\n",
        "    hvp_norm = torch.norm(hvp_flat, p=2) # L2 norm\n",
        "    if hvp_norm > max_norm:\n",
        "        hvp_flat = hvp_flat * (max_norm / hvp_norm)\n",
        "\n",
        "    # Debugging: check for explosion\n",
        "    if torch.norm(hvp_flat) > 1e6:\n",
        "        print(f\"\\nExploding values detected in HVP after clipping! Norm: {torch.norm(hvp_flat)}\")\n",
        "\n",
        "    return hvp_flat\n",
        "\n",
        "\n",
        "def lissa_approximation(model, train_dataloader, v, damping=0.1, num_samples=5, num_iter=100, num_repeats=5, criterion=None):\n",
        "    \"\"\"\n",
        "    Approximates Hessian-inverse-vector product (iHVP) using the stochastic estimation method\n",
        "    explained in https://arxiv.org/pdf/1703.04730 and https://arxiv.org/pdf/1602.03943\n",
        "\n",
        "    Parameters:\n",
        "    - model: Pre-trained model\n",
        "    - train_dataloader: Dataloader for training data\n",
        "    - v: Gradient vector\n",
        "    - damping: Damping factor for stabilization\n",
        "    - num_samples: Number of training points (t) to sample per iteration\n",
        "    - num_iter: Number of Taylor approximation iterations\n",
        "    - num_repeats: Number of times to repeat estimation to reduce variance (r)\n",
        "\n",
        "    Returns:\n",
        "    - Approximate inverse Hessian-vector product (iHVP)\n",
        "    \"\"\"\n",
        "    ihvp_estimates = []\n",
        "\n",
        "    for i in range(num_repeats):\n",
        "        # H^{-1}_0 v = v\n",
        "        print(f\"Repeat {i+1}/{num_repeats}\")\n",
        "        z = v.clone()\n",
        "\n",
        "        # sampling training points\n",
        "        print(f\"Dataset length: {len(train_dataloader.dataset)}\")\n",
        "        print(f\"Dataset type: {type(train_dataloader.dataset)}\")\n",
        "        indices = torch.randint(len(train_dataloader.dataset), (num_samples,)).tolist()\n",
        "        sampled_train_data = [train_dataloader.dataset[i] for i in indices]\n",
        "\n",
        "        # taylor approximation\n",
        "        for j in range(num_iter):\n",
        "            print(f\"Iteration {j+1}/{num_iter}\")\n",
        "            train_batch = sampled_train_data[j % num_samples] # Filtering the sampled train instances\n",
        "            train_input_ids = torch.stack([item['input_ids'] for item in sampled_train_data]).to(device)\n",
        "            train_attention_mask = torch.stack([item['attention_mask'] for item in sampled_train_data]).to(device)\n",
        "            train_label = torch.stack([item['target'] for item in sampled_train_data]).to(device)\n",
        "\n",
        "            # Compute Hessian-gradient product using the sampled loss\n",
        "            train_loss = criterion(\n",
        "                model(train_input_ids, train_attention_mask).view(-1), train_label\n",
        "            )\n",
        "            hvp = compute_hvp(model, train_loss, z)\n",
        "\n",
        "            # update: H_j^{-1} v = v + (I - H) H_{j-1}^{-1} v\n",
        "            z = v + (z - hvp)\n",
        "\n",
        "        ihvp_estimates.append(z)\n",
        "\n",
        "    return torch.stack(ihvp_estimates).mean(dim=0)\n",
        "\n",
        "def influence_function(train_point, train_label, grad_test_vector, model, criterion, train_dataloader, num_samples=5, num_iter=5, num_repeats=5):\n",
        "    \"\"\"\n",
        "    Computes the influence of a training point over full test set using stochastic method\n",
        "\n",
        "    Parameters:\n",
        "    - train_point: Dictionary containing {'input_ids': tensor, 'attention_mask': tensor}\n",
        "    - train_label: Target label for the training point\n",
        "    - grad_test_vector: Precomputed gradient of test loss w.r.t. model parameters\n",
        "    - model: Pre-trained model\n",
        "    - criterion: Loss function (MSELoss)\n",
        "    - train_dataloader: Dataloader for training data\n",
        "    - num_samples: Number of training points (t) to sample per iteration\n",
        "    - num_iter: Number of Taylor approximation iterations\n",
        "    - num_repeats: Number of times to repeat estimation (r)\n",
        "\n",
        "    Returns:\n",
        "    - Influence of the training point on the test set\n",
        "    \"\"\"\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Compute gradient of training loss w.r.t. model parameters\n",
        "    train_loss = criterion(model(train_point['input_ids'], train_point['attention_mask']).view(-1), train_label)\n",
        "    grad_train = torch.autograd.grad(train_loss, model.parameters(), retain_graph=True)\n",
        "    grad_train_vector = torch.cat([g.view(-1) for g in grad_train])\n",
        "\n",
        "    # Compute Hessian-inverse-vector product using LiSSA\n",
        "    print(\"computing ihvp\")\n",
        "    ihvp = lissa_approximation(model, train_dataloader, grad_train_vector, num_samples=num_samples, num_iter=num_iter, num_repeats=num_repeats, criterion=criterion)\n",
        "\n",
        "    # Compute influence using dot product\n",
        "    influence = torch.dot(grad_test_vector, ihvp)\n",
        "\n",
        "    return -influence"
      ],
      "metadata": {
        "id": "KhuXtwjV90jJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up Model and Data"
      ],
      "metadata": {
        "id": "LIyuEsZ4-JoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"  #MoLFormer model\n",
        "DATASET_PATH = \"scikit-fingerprints/MoleculeNet_Lipophilicity\"\n",
        "\n",
        "# load pre-trained model from HuggingFace\n",
        "model = AutoModel.from_pretrained(MODEL_NAME, deterministic_eval=True, trust_remote_code=True)\n",
        "\n",
        "# load the fine-tuned masked model from task-1\n",
        "# for mouting drive in google colab\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/Colab Notebooks/nnti/'\n",
        "os.chdir(path)\n",
        "mlm_finetuned_model = AutoModel.from_pretrained(\"./mlm_finetuned_model\", local_files_only=True, trust_remote_code=True).to(device) # fine tuned model\n",
        "mlm_regression_model = MoLFormerWithRegressionHead(mlm_finetuned_model).to(device) # initialize with regression head\n",
        "# reset the path to git repo\n",
        "os.chdir(\"/content/nnti-project-25/\")\n",
        "print(os.getcwd())\n",
        "\n",
        "# load dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "dataset = load_dataset(DATASET_PATH)\n",
        "\n",
        "# loading external dataset\n",
        "ext_data = pd.read_csv(\"./tasks/External-Dataset_for_Task2.csv\")\n",
        "ext_data = ext_data.iloc[0:5] # select a subset from external dataset\n",
        "ext_data = ext_data.rename(columns={\"Label\": \"label\"}) # making column names consistent\n",
        "ext_dataset = HF_Dataset.from_pandas(ext_data)\n",
        "ext_dataset = ext_dataset.remove_columns([\"__index__\"]) if \"__index__\" in ext_dataset.column_names else ext_dataset\n",
        "\n",
        "# train-test-val split\n",
        "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42) # 80:20\n",
        "train_valid_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n",
        "split_train_valid = train_valid_dataset.train_test_split(test_size=0.1, seed=42) # 90:10\n",
        "train_dataset = split_train_valid[\"train\"]\n",
        "valid_dataset = split_train_valid[\"test\"]\n",
        "combined_train = concatenate_datasets([train_dataset, ext_dataset])\n",
        "\n",
        "# create dataset and dataloader\n",
        "train_dataset = SMILESDataset(train_dataset, tokenizer, max_length=128)\n",
        "valid_dataset = SMILESDataset(valid_dataset, tokenizer, max_length=128)\n",
        "test_dataset  = SMILESDataset(test_dataset, tokenizer, max_length=128)\n",
        "ext_dataset = SMILESDataset(ext_dataset, tokenizer, max_length=128)\n",
        "combined_train = SMILESDataset(combined_train, tokenizer, max_length=128)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "test_dataloader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "combined_train_dataloader = DataLoader(combined_train, batch_size=16, shuffle=True)\n",
        "# ext_train_dataloader = DataLoader(ext_dataset, batch_size=16, shuffle=False) # for training the model - batch size 16\n",
        "ext_influence_dataloader = DataLoader(ext_dataset, batch_size=1, shuffle=False) # for determing influence of each train points - batch size 1"
      ],
      "metadata": {
        "id": "4op_LO4_-Isx",
        "outputId": "94b3a239-bf0b-4720-8f58-52f652c7e0bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/nnti-project-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Xy4ghiO3BOyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS_reg = 4\n",
        "LEARNING_RATE_reg = 1e-7\n",
        "optimizer_reg = torch.optim.Adam(mlm_regression_model.parameters(), lr=LEARNING_RATE_reg)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for epoch in range(EPOCHS_reg):\n",
        "    mlm_regression_model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    # training with combined set\n",
        "    for batch in combined_train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        mask = batch['attention_mask'].to(device)\n",
        "        targets = batch['target'].to(device)\n",
        "\n",
        "        optimizer_reg.zero_grad()\n",
        "        outputs = mlm_regression_model(input_ids, mask)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer_reg.step()\n",
        "\n",
        "        total_train_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataset)\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation phase inside the epoch loop using validation set from original dataset\n",
        "    mlm_regression_model.eval()\n",
        "    total_valid_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            mask = batch['attention_mask'].to(device)\n",
        "            targets = batch['target'].to(device)\n",
        "            outputs = mlm_regression_model(input_ids, mask)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            total_valid_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_valid_loss = total_valid_loss / len(valid_dataset)\n",
        "    print(f\"Epoch {epoch+1} - Validation Loss: {avg_valid_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "bP7NStZ1BUEc",
        "outputId": "b5a8997a-912e-47ae-c42c-e717ff14b34d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Train Loss: 5.0597\n",
            "Epoch 1 - Validation Loss: 4.9108\n",
            "Epoch 2 - Train Loss: 4.3090\n",
            "Epoch 2 - Validation Loss: 4.0798\n",
            "Epoch 3 - Train Loss: 3.6379\n",
            "Epoch 3 - Validation Loss: 3.4412\n",
            "Epoch 4 - Train Loss: 3.0849\n",
            "Epoch 4 - Validation Loss: 2.8947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Influence Checking"
      ],
      "metadata": {
        "id": "X6A5CJQ6CuYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "influences = []\n",
        "criterion = nn.MSELoss()\n",
        "regression_model = mlm_regression_model\n",
        "\n",
        "# compute loss over full test set\n",
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "test_labels = []\n",
        "print(\"Computing test loss gradient...\")\n",
        "grad_test_accum = None\n",
        "num_test_samples = 0\n",
        "for test_batch in test_dataloader:\n",
        "    batch_input_ids = test_batch['input_ids'].to(device)\n",
        "    batch_attention_mask = test_batch['attention_mask'].to(device)\n",
        "    batch_labels = test_batch['target'].to(device)\n",
        "\n",
        "    test_loss = criterion(regression_model(batch_input_ids, batch_attention_mask).squeeze(), batch_labels)\n",
        "    grad_test = torch.autograd.grad(test_loss, regression_model.parameters(), retain_graph=True)\n",
        "    grad_test_vector = torch.cat([g.view(-1) for g in grad_test])\n",
        "\n",
        "    if grad_test_accum is None:\n",
        "        grad_test_accum = grad_test_vector.clone()\n",
        "    else:\n",
        "        grad_test_accum += grad_test_vector\n",
        "\n",
        "    num_test_samples += batch_labels.shape[0]\n",
        "grad_test_vector = grad_test_accum / num_test_samples\n",
        "\n",
        "# Compute influence for each sample in external dataset\n",
        "for train_batch in ext_influence_dataloader:\n",
        "    print(f\"External data sample {len(influences) + 1}/{len(ext_influence_dataloader)}\")\n",
        "    train_input_ids = train_batch['input_ids'].to(device)\n",
        "    train_attention_mask = train_batch['attention_mask'].to(device)\n",
        "    train_label = train_batch['target'].to(device)\n",
        "\n",
        "    train_point = {\n",
        "        'input_ids': train_input_ids,\n",
        "        'attention_mask': train_attention_mask\n",
        "    }\n",
        "\n",
        "    # Compute influence\n",
        "    influence = influence_function(\n",
        "        train_point, train_label, grad_test_vector,\n",
        "        regression_model, criterion, train_dataloader\n",
        "    )\n",
        "\n",
        "    influences.append(influence.item())\n",
        "    print(f\"Influence for current training batch: {influence.item()}\")\n",
        "\n",
        "# Rank external data points by influence\n",
        "# ranked_indices = sorted(range(len(influences)), key=lambda i: influences[i], reverse=True)\n",
        "ranked_indices = sorted(enumerate(influences), key=lambda x: x[1], reverse=True) # sorted by influence score\n",
        "print(\"Most influential training points:\", ranked_indices)\n"
      ],
      "metadata": {
        "id": "eOeIySV6C0ci",
        "outputId": "d4a664e8-133e-4362-9519-a918d4a77c0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing test loss gradient...\n",
            "External data sample 1/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: -945.623291015625\n",
            "External data sample 2/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: -1557.5968017578125\n",
            "External data sample 3/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: 472.48638916015625\n",
            "External data sample 4/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: -1253.0889892578125\n",
            "External data sample 5/5\n",
            "computing ihvp\n",
            "Repeat 1/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 2/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 3/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 4/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Repeat 5/5\n",
            "Dataset length: 3024\n",
            "Dataset type: <class '__main__.SMILESDataset'>\n",
            "Iteration 1/5\n",
            "Iteration 2/5\n",
            "Iteration 3/5\n",
            "Iteration 4/5\n",
            "Iteration 5/5\n",
            "Influence for current training batch: -1012.5614624023438\n",
            "Most influential training points: [(2, 472.48638916015625), (0, -945.623291015625), (4, -1012.5614624023438), (3, -1253.0889892578125), (1, -1557.5968017578125)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Garbage Cleaning"
      ],
      "metadata": {
        "id": "KwlEt-ylDOuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del regression_model\n",
        "del train_dataset\n",
        "del test_dataset\n",
        "del train_dataloader\n",
        "del test_dataloader\n",
        "del ext_dataset\n",
        "del ext_dataloader\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "aqszc-zLCuSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}